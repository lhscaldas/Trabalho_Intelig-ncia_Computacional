Leia o enunciado abaixo e verifique se as três classes python depois dele estão corretas para resolver o problema:


Neste problema, você criará a sua própria função target $f$ e uma base de dados $D$ para que possa ver como o
Algoritmo de Aprendizagem Perceptron funciona. Escolha $d = 2$ pra que você possa visualizar o problema,
e assuma $\chi = [-1, 1] \times [-1, 1]$ com probabilidade uniforme de escolher cada $x \in \mathcal{X}$ .

Em cada execução, escolha uma reta aleatória no plano como sua função target $f$ (faça isso selecionando dois pontos aleatórios, uniformemente distribuídos em  $\chi = [-1, 1] \times [-1, 1]$, e pegando a reta que passa entre eles), de modo que um lado da reta mapeia pra +1 e o outro pra -1. Escolha os inputs $x_n$ da base de dados como um conjunto de pontos aleatórios (uniformemente em $ \mathcal{X}$ ), e avalie a função target em cada $x_n$ para pegar o output correspondente $y_n$.

Agora, pra cada execução, use o Algoritmo de Aprendizagem Perceptron (PLA) para encontrar $g$. Inicie o PLA com um vetor de pesos $w$ zerado (considere que $sign(0) = 0$, de modo que todos os pontos estejam classificados erroneamente ao início), e a cada iteração faça com que o algoritmo escolha um ponto aleatório dentre os classificados erroneamente. Estamos interessados em duas quantidades: o número de iterações que o PLA demora para convergir pra $g$, e a divergência entre $f$ e $g$ que é $\mathbb{P}[f (x) \neq g(x)]$ (a probabilidade de que $f$ e $g$ vão divergir na classificação de um ponto aleatório). Você pode calcular essa probabilidade de maneira exata, ou então aproximá-la ao gerar uma quantidade suficientemente grande de novos pontos para estimá-la (por exemplo, 10.000).

A fim de obter uma estimativa confiável para essas duas quantias, você deverá realizar 1000 execuções do experimento (cada execução do jeito descrito acima), tomando a média destas execuções como seu resultado
final.

Para ilustrar os resultados obtidos nos seus experimentos, acrescente ao seu relatório gráficos scatterplot
com os pontos utilizados para calcular $E_{out}$, assim como as retas correspondentes à função target e à hipótese $g$ encontrada.

# Classe para criar a função target
class Target:
    def __init__(self): 
        self.a = 0 # coeficiente angular
        self.b = 0 # coeficiente linear

    # Método para gerar a linha da função target
    def generate_random_line(self):
        point1 = np.random.uniform(-1, 1, 2) # ponto aleatorio no domínio
        point2 = np.random.uniform(-1, 1, 2) # ponto aleatorio no domínio
        a = (point2[1] - point1[1]) / (point1[0] - point2[0]) # cálculo do coeficiente angular
        b = point1[1] - a*point1[0] # cálculo do coeficiente linear
        self.a = a
        self.b = b
        return a, b
    
    # Método para classificar pontos de acordo a função target
    def classify_point(self, point):
        a = self.a
        b = self.b
        y_reta = a*point[0] + b    
        return np.sign(point[1] - y_reta) # verifica se a coordenada y do ponto está acima ou abaixo da reta

# Classe para criar o dataset
class Dataset:
    def __init__(self, N): 
        self.N = N # tamanho do dataset

    # Método para gerar a base de dados D
    def generate_dataset(self, target):
        N = self.N
        data = np.random.uniform(-1, 1, (N, 2)) # gera N pontos no R2 com coordenadas entre [-1, 1]
        labels = np.array([target.classify_point(point) for point in data])
        return data, labels

# Classe para criar e treinar o perceptron 2D
class Perceptron2D:
    def __init__(self, weights = np.zeros(3)):
        self.w = weights # inicializa os pesos (incluindo o w_0)
    
    # Método para treinar o perceptron usando o algoritmo de aprendizagem perceptron (PLA)
    def pla(self, data, labels): 
        n_samples = len(data)
        X_bias = np.hstack([np.ones((n_samples, 1)), data]) # adiciona uma coluna de 1s para o X_0 (coordenada artificial)
        iterations = 0
        errors = 1
        while errors > 0:
            errors = 0
            for i in range(n_samples):
                if labels[i] * np.dot(self.w, X_bias[i]) <= 0:
                    self.w += labels[i] * X_bias[i] # atualiza os pesos
                    errors += 1
            iterations += 1
        return iterations, self.w
    
    # Método para classificar um dataset com base nos pesos aprendidos.
    def classificar(self, data):
        n_samples = len(data)
        X_bias = np.hstack([np.ones((n_samples, 1)), data]) # adiciona uma coluna de 1s para o bias X_0
        return np.sign(np.dot(X_bias, self.w)) # verifica o sinal do produto escalar entre x e w


        Agora verifique se o item abaixo pode ser respondido pela função depois dele

        \item Considere $N = 10$. Quantas iterações demora, em média, para que o PLA convirja com $N = 10$
        pontos de treinamento? Escolha o valor mais próximo do seu resultado.
    
        \begin{enumerate}
            \item[\textcolor{red}{(a)}]\textcolor{red}{1}\addtocounter{enumii}{1}
            \item 15 *
            \item 300
            \item 5000
            \item 10000
        \end{enumerate}

        def calc_num_iter(num_points, verbose = True):
            lista_iter = list()
            for _ in range(1000):
                target = Target()
                target.generate_random_line()
                dataset = Dataset(num_points)
                data, labels = dataset.generate_dataset(target)
                perceptron = Perceptron2D()
                iter, _ = perceptron.pla(data,labels)
                lista_iter.append(iter)
            if verbose: print(f"{np.mean(lista_iter)} iterações com desvio padrão {np.std(lista_iter):.4f} (min:{np.min(lista_iter)}, máx:{np.max(lista_iter)})")
            return lista_iter


